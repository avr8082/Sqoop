sqoop 1.4.6 
Sqoop is to get the data from RDBMS using jdbc into HDFS or other databases.
Sqoop import-all-tables will import all tables once from mysql database or Oracle schema

$ hadoop fs -ls /user 
$ ps -ef|grep -i manager 
$ ps -ef|grep -i node 

ps -fu hdfs --> to see name node, data node, secondary name node running or not
ps -fu yarn --> to see YARN nodemanager & resource manager running or not 

# Connecting to Mysql 

mysql -u retail_dba -p ENTER 
password = cloudera 
mysql>
mysql> use retail_db;
mysql> show tables;
exit
$ sqoop help 
$ hadoop fs -ls /user/cloudera --> "/user/cloudera" is the name space. This command also gives the list of the directories in this name space 
$ hadoop fs -mkdir /user/cloudera/sqoop_import --> this will create a directory called sqoop_import in the name space /user/cloudera 

The below stmt is to connect to mysql using retail_dba user with password cloudera

$ sqoop list-databases --connect "jdbc:mysql://gw01.itversity.com:3306" --username retail_dba --password itversity
$ sqoop list-databases \
 --connect "jdbc:mysql://quickstart.cloudera:3306" \
 --username retail_dba \
 --password cloudera

# quickstart.cloudera = Host Name 

$ sqoop list-tables \
 --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
 --username retail_dba \
 --password cloudera

Before running import command, we have to evaluate OR validate the necessary permissions on tables using eval command like below:
$ sqoop eval \
 --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
 --username retail_dba \
 --password cloudera \
 --query "select * from departments limit 10"  OR --query "describe orders"
 
 
 
It will provide the list of the tables in mysql database "retail_db"

****** HELP ***********
$ sqoop help eval 
$ sqoop help list-databases 


******  ERRORS ******
1. If any error with hostname, check with admins what is base location for the jar files
2. If could not load data with oracle, means JDBC jar file is missing 
3. Connection Refused (Solution : either hostname or port # correctly. Provide correct hostname & port #) 
Validate hostname and port # using telnet command like below:
$ telnet gw01.itversity.com 3306 
Connected to gw01.itversity.com 3306 
If everything correct, there must be FIREWALL blocking.Check with Admins.
OR $ nc gw01.itversity.com 3306 

4. Access denied for user "gw01.itversity.com " -- In this case incorrect user name or password provided as part of script.
5. Sqoop is generic tool. It will work with many databases. With Oracle 12c , trying to database as multi tenant. 
If you run the same the same command with list-databases, it will run the list of the schemas NOT list of the tables. This is difference.So, the behavior is little different with different databases.



Google search in Sqoop user guide 
versions Sqoop & Sqoop 2
Good doc is V1.2

****SQOOP IMPORT ********

$ sqoop import --help -->    for help on import arguments
$ hostname -f -->     to find the hostname. here output is "quickstart.cloudera"
*** Never sqoop import into your home directory.

# import-all-tables 
$ sqoop import-all-tables \
   -m 12 \
  --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
  --username=retail_dba \
  --password=cloudera \
  --warehouse-dir=/user/cloudera/sqoop_import 
 
 
 
 *** ONE TABLE IMPORT
 $ sqoop import \
   --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
   --username retail_dba \
   --password cloudera \
   --table orders \
  --target-dir /user/cloudera/sqoop_import/orders
  
Target Dir = You need to provide the base directory where data needs to be copied. Here, directory is /user/cloudera/sqoop_import/orders
Warehouse Dir = Directory will be created as table name and copied. Means , we MUST provide base directory like: /user/cloudera/sqoop_import ONLY (WTTHOUT TABLE NAME like below). It will create the sub directory with table name under base directory

***USING WAREHOUSE DIRECTORY
 $ sqoop import \
   --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
   --username retail_dba \
   --password cloudera \
   --table orders \
  --warehouse-dir /user/cloudera/sqoop_import
  
 ***USING --append 
 #This will append the files with the existing files 
 hadoop fs -ls /user/cloudera/sqoop_import/orders/part*
 BEFORE: 4 files 
 AFTER: 8 files 
 
  $ sqoop import \
   --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
   --username retail_dba \
   --password cloudera \
   --table orders \
  --warehouse-dir /user/cloudera/sqoop_import \
  --append 
  
  ***USING --delete-target-dir (It will delete the target directory and copy 4 files. 4 files due to number of default mappers using --numb-mappers)
   $ sqoop import \
   --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
   --username retail_dba \
   --password cloudera \
   --table orders \
  --warehouse-dir /user/cloudera/sqoop_import \
  --delete-target-dir
   
  IT will generate the code, using column names,data types of the columns, then only it will write into HDFS. Next, it will create .java file and copy data. 
  Bouding vals query use min value(1) and max value(68883 for example) into 4 buckets/splits. 68883/4 = 17220. We can see this info in sqoop job log. Each thread will get mutually exclusive data. 
 
 
$ hadoop fs -ls /user/cloudera/sqoop_import --> to see imported tables 
$ hadoop fs -ls /user/cloudera/sqoop_import/departments --> to see the files of tables ( for ex: /user/cloudera/sqoop_import/order_items/part-m-00011) 
$ hadoop fs -tail /user/cloudera/sqoop_import/order_items/part-m-00011  # to see the data in this file 
$ hadoop fs -cat /user/cloudera/sqoop_import/order_items/part-m-*|wc -l # to see the records count
$ hadoop fs -rm -R /user/cloudera/sqoop_import/orders 

$ sqoop eval \
  --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
  --username retail_dba \
  --password cloudera \
  --query "select count(1) from order_items" --> to find the number of records using eval command
  
  Sqoop supports 4 file formats 
  1. Avro (--as-avrodatafile) --> it will give you avsc file with metadata  
  2. Text (--as-textfile) --> this will have delimiter 
  3. Sequence (--as-sequencefile) --> this also capture metadata 
  4. Parquet (--as-parquetfile)
  
  For older clusters, parquet file may not work some times
  
    $ sqoop import \
   --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
   --username retail_dba \
   --password cloudera \
   --table orders \
  --warehouse-dir /user/cloudera/sqoop_import \
  --delete-target-dir \
  --as-sequencefile 
  
  Meta data file = orders.java 
  
  $ cat orders.avsc 
  
  Smoke testing = UAT team test the entire process with test data after deployment in production 
  
******************      https://www.youtube.com/watch?v=x1u5Fdppvd8 - Hadoop Certification - 02 Sqoop Import   ******************


# Avro file format is similar to JSON file format
# If no file format given, by default is text file 
# Warehouse directory is /user/cloudera/sqoop_import  --> this is the directory we created in HDFS to store mysql tables
# BoundaryValQuery will apply min & max values of primary key values of the table, using that it will divide into exclusive buckets. Playlist building enterprise 	using hadoop using boundary queries . To addres the skew, we use boundary query.

# .-m 12 is number of mappers is used to split each table into 12 threads and import data into HDFS. As we have 6 tables, it will run 6 MR jobs each with 12 threads to get the data into HDFS to warehouse directory which we specified 
 i. By default number of mappers are 4   
 i. comma is the default delimiter of the data 
 i. Run the eval command always to evaluate the data in HDFS instead of logging into MySQL

*********  https://www.youtube.com/watch?v=x1u5Fdppvd8 : Hadoop Certification - 02 Sqoop Import ********
$ hadoop fs -ls /user/hive/warehouse ---> This is also considered as HIVE default database. The out put this command we can get the directories with db extn and w/o db extn

$ hive 
hive> create database sqoop_import;
hive> dfs -ls /user/hive/warehouse --> Here, we used dfs command instead of "hadoop fs" --> OP is list of the tables under this directory including sqoop_import table
# we no need to have warehouse directory when we run the import command 

$ sqoop import-all-tables \
 --num-mappers 1 \
 --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
 --username=reatil_dba \
 --password=cloudera \
 --hive-import \
 --hive-overwrite \
 --create-hive-table \
 --compress \
 --compresession-codec org.apache.hadoop.io.compress.SnappyCodec \
 --outdir java_files 

# connect to hive in diff terminal and use the below commands 
hive>show tables;
hive>describe formatted departments;
#this will give the location of the table departments like hdfs://quickstart.cloudera:8020/user/hive/warhouse/departments 
# THEN run below command
hive>dfs -ls /user/hive/warehouse/departments
#this will show the path of the file for departments in the form of /user/hive/warhouse/departments/part-m-00000.snappy

hive>dfs -du -s -h /user/hive/warehouse/order_items
# this will show the size of the file 

hive>dfs -tail /user/hive/warehouse/order_items/part-m-00000.snappy;
# this will show the data of this file part-m-00000.snappy in garbled format

hive>select count(1) from departments;
# to see the count of the records in departments tables ALSO run the /eval command in mysql database to find the number of records. Basically we try to match number of records from MySQL = Hive table

# hadoop fs --> Linux interface commands 
# dfs --> Hive Command Line Interface (CLI) commands

****************   https://www.youtube.com/watch?v=7oZ_CctyS5Q - Hadoop Certification - 03 Sqoop Import  ************************

ERRORS:
1. Listener Error / connection refused - Ans is Check if HDFS is running in cloudera manager
2. Check if HDFS is in safemode 

# Basic import 

sqoop import \
--connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
--username=retail_dba \
--password=cloudera \
--table departments \
--target-dir /user/cloudera/departments

$ hadoop fs -cat /user/cloudera/departments/part*
$ hadoop fs -cat /user/cloudera/departments/part-m-00001 --> to see specific file 

# insert new records in departments table 
$ sqoop eval \
 --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
 --username retail_dba \
 --password cloudera \
 --query "insert into departments value (8000,'TESTING')"

# remove the file 
$hadoop fs -rm -R /user/cloudera/departments THEN RUN IMPORT AGAIN with 2 mappers 
# import again with table argument
sqoop import \
--connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
--username=retail_dba \
--password=cloudera \
--table departments \
-m 2
--target-dir /user/cloudera/departments 

# Using boundary query 
sqoop import \
--connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
--username=retail_dba \
--password=cloudera \
--table departments \
-m 2
--target-dir /user/cloudera/departments 
--boundary-query "select min(department_id), max(department_id) from departments where department_id<>8000"

$ hadoop fs -ls /user/cloudera/departments 
# here we can see data evenly distributed among the file. Before boundary query , data skewed into unevenly into file. 

# Boundary query custom - It's very imp to supersede the importance of the query parameters 
sqoop import \
--connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
--username=retail_dba \
--password=cloudera \
--table departments \
-m 2
--target-dir /user/cloudera/departments 
--boundary-query "select 2, 8 from departments limit 1"

sqoop import \
--connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
--username=retail_dba \
--password=cloudera \
--table orders \
-m 2
--target-dir /user/cloudera/orders 
--boundary-query "select min(order_id), max(order_id) from orders where order_id between 11111 and 70000"

or --boundary-query "select 111111,68883"  #hot coded here 



# with --columns control argument. This will import only the required columns
sqoop import \
--connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
--username=retail_dba \
--password=cloudera \
--table departments \
-m 2
--target-dir /user/cloudera/departments 
--boundary-query "select 2, 8 from departments limit 1"
--columns department_id,department_name

# --table & --query are mutually exclusive. We cannot use both in the same script

# --split-by
$ sqoop import \
--connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
--username=retail_dba \
--password=cloudera \
--table departments \
-m 2
--target-dir /user/cloudera/departments 
--boundary-query "select 2, 8 from departments limit 1"
--split-by department_id
(you use indexed column with --split-by) 
If it is numberic it will pass, if it is string, we have to bypass "-Dorg.apache.sqoop.splitter.allow_text_splitter=true" parameter. By default, --split-by will expect numberic column to be given (for ex: department_id / order_id)

*** USING -Dorg.apache.sqoop.splitter.allow_text_splitter=true 
$ sqoop import \
--connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
--username=retail_dba \
--password=cloudera \
--table orders \
--warehouse-dir /user/cloudera/orders \
--delete-target-dir \
--split-by order_status \
-Dorg.apache.sqoop.splitter.allow_text_splitter=true
##### with this parameter it is not working ######


# Demonstrate --split by 
# connect to hive 
$ hive -u retail_dba -p ENTER 
# enter password 
hive>use retail_db;
hive>create table departments_nopk as select * from departments;

# run the basic import with table "departments_nopk"

sqoop import \
--connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
--username=retail_dba \
--password=cloudera \
--table departments_nopk \
--target-dir /user/cloudera/departments



#--query --> if we have 2 tables in source and import into HDFS. Make sure that we have $CONDITIONS to escape where condition
sqoop import \
--connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
--username=retail_dba \
--password=cloudera \
--query="select * from orders join order_items on orders.order_id =order_items.order_item_order_id where \$CONDITIONS" \
--target-dir /user/cloudera/order_join \
--split-by order_id \
--num-mappers 4 

# -- Getting delta (--where)
sqoop import \
  --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
  --username=retail_dba \
  --password=cloudera \
  --table departments \
  --target-dir /user/hive/warehouse/retail_ods.db/departments \
  --append \
  --fields-terminated-by '|' \
  --lines-terminated-by '\n' \
  --split-by department_id \
  --where "department_id > 7" \
  --outdir java_files

*******************  https://www.youtube.com/watch?v=HOzju75gNQs - Hadoop Certification - 04 Sqoop Import  *********************

# Demo on importing individual tables into HIVE
# we can create the table while import & we can create the table then import
# connect Hive 
# create database sqoop_import and use sqoop_import 
hive> create table departments (department_id int, department_name string);
hive> describe formatted departments;
# with describe formatted departments command , we can find the target directory in Hive

# Importing table departments into Hive using Sqoop with hive-overwrite option
$ sqoop import \
  --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
  --username=retail_dba \
  --password=cloudera \
  --table departments \
  --hive-home /user/hive/warehouse \
  --hive-import \
  --hive-overwrite \
  --hive-table sqoop_import.departments \
  --outdir java_files 

# When data moved from MySQL to Hive, it will store in TEMP DIRECTORY of the user which is /user/cloudera
$ hadoop fs -rm -R /user/cloudera/departments (if required)
# Remove the directory /user/cloudera/departments if it already exist
# Basically we imported data from MySQL to /user/hive/warehouse/sqoop_import.db/departments 

# we can also append to table by using --hive-import  *** ERROR **
$ sqoop import \
  --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
  --username=retail_dba \
  --password=cloudera \
  --table departments \
  --hive-home /user/hive/warehouse \
  --hive-import \
  --hive-table sqoop_import.departments_test \
  --create-hive-table \
  --outdir java_files 

$ sqoop import \
  --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
  --username=retail_dba \
  --password=cloudera \
  --table materials \
  --hive-home /user/hive/warehouse \
  --hive-import \
  --hive-table sqoop_import.materials \
  --create-hive-table \
  --outdir java_files 



sqoop import \
  --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
  --username=retail_dba \
  --password=cloudera \
  --table departments \
  --hive-home /user/hive/warehouse \
  --hive-import \
  --hive-table sqoop_import.departments \
  --outdir java_files 

************  https://www.youtube.com/watch?v=ntSK_oJtWlQ -  05 Sqoop Import Incremental ************

# run the eval command to check the data in departments table 
$ sqoop eval \
 --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
 --username retail_dba \
 --password cloudera \
 --query "select * from departments"

# to capture the delta/incremental records WITH --append 
$ sqoop import \
  --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
  --username=retail_dba \
  --password=cloudera \
  --table departments \
  --target-dir /user/cloudera/sqoop_import/departments \
  --append \
  --where "department_id > 7" \
  --outdir java_files

# we have to see log file for last value.This command will load DUPLICATE records because of --append argument

$ sqoop import \
  --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
  --username=retail_dba \
  --password=cloudera \
  --table departments \
  --target-dir /user/cloudera/sqoop_import/departments \
  --append \
  --check-column "department_id" \
  --incremental append \
  --last-value 7 \
  --outdir java_files


# Incremental load with check-column/incremental/last-value - Yet to check with lastmodified option
$ sqoop import \
  --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
  --username=retail_dba \
  --password=cloudera \
  --table departments \
  --target-dir /user/cloudera/sqoop_import/departments \
  --append \
  --check-column "department_id" \
  --incremental lastmodified \
  --last-value 8000 \
  --outdir java_files

************  https://www.youtube.com/watch?v=GyA6lhBIe9g - 01 Sqoop Export (Export from HDFS to MySQL) ******************************

# Typical export will insert into target table.Update can be done with update (update-key, update-mode) arguments
# 1. Create one table in MySQL with where 1=2 condition so it will not insert any records but it will create the table order_items_export.
mysql> create table order_items_export as select * from order_items where 1=2;

# export from /user/cloudera/sqoop_import/order_items data into mySQL table order_items_export 
# sqoop export doesnot show any diff between Hive and HDFS. Both are treated in similar fashion. 
# None of the imprort parameters will not be available in export 

# Simple export
$ sqoop export \
  --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
  --username=retail_dba \
  --password=cloudera \
  --table order_items_export \
  --export-dir /user/cloudera/sqoop_import/order_items \
  --batch \
  --outdir java_files

*************  https://www.youtube.com/watch?v=laMBRY9TJDA - 02 Sqoop Export - Merge/Upsert  *******************************************
# create a local file and merge with the existing data in departments table
$ vi departments_export 
# enter some records 
$ cat departments_export 
# create a directory departments under /user/cloudera/sqoop_import
$ hadoop fs -mkdir /user/cloudera/sqoop_import/departments_export 

# merge the local file departments_export under "/home/cloudera/departments_export" into "/user/cloudera/sqoop_import/departments_export"

$ hadoop fs -put /home/cloudera/departments_export /user/cloudera/sqoop_import/departments_export
# moving data to export.csv file 
$ hadoop fs -mv /user/cloudera/sqoop_import/departments_export/departments_export /user/cloudera/sqoop_import/departments_export/export.csv

# update-mode allowinsert is to simulate merge or upsert operation 
$ sqoop export \
  --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
  --username=retail_dba \
  --password=cloudera \
  --table departments \
  --export-dir /user/cloudera/sqoop_import/departments_export \
  --batch \
  --outdir java_files \
  -m 1 \
  --update-key department_id \
  --update-mode allowinsert 

# after the above script, login to mySQL then check the departments table or run eval command in HDFS to check the data updated 
$ sqoop eval \
 --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
 --username retail_dba \
 --password cloudera \
 --query "select * from departments"

# We updated the vi file with 2 records like 7, Fan Shop & 10000 testing export. Here, WITHOUT --update-mode allowinsert argument, only record with department_id=7 updated and 10000 will not be updated. In general WITHOUT updatemode argument, by default it is with only update

$ sqoop export \
  --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
  --username=retail_dba \
  --password=cloudera \
  --table departments \
  --export-dir /user/cloudera/sqoop_import/departments_export \
  --batch \
  --outdir java_files \
  -m 1 \
  --update-key department_id \


sqoop export \
  --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
  --username=retail_dba \
  --password=cloudera \
  --table departments_export \
  --export-dir /user/cloudera/sqoop_import/departments_export \
  --batch \
  --outdir java_files \
  -m 1 \
  --update-key department_id \
  --update-mode allowinsert 

# if the table doesn't have primary key, DUPLICATES will be inserted 
# create the table in mysql with no primary key and run the export script with update-mode allowinsert.DUPLICATE records will be inserted into table departments_export since we dont have the primary key. That's why it will insert records
mysql> create table departments_export as select * from departments;

$ sqoop export \
>   --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
>   --username=retail_dba \
>   --password=cloudera \
>   --table departments_export \
>   --export-dir /user/cloudera/sqoop_import/departments_export \
>   --batch \
>   --outdir java_files \
>   -m 1 \
>   --update-key department_id \
>   --update-mode allowinsert

# --staging-table (if we don't want to directly update the destination table, we can write into staging table first)


# --enclosed-by (data will be enclosed in double quotes)
# syntax = --enclosed-by <char>  --> set a required field enclosing char (like "10")
$ sqoop import \
--connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
--username retail_dba \
--password cloudera \
--table departments \
--target-dir /user/cloudera/sqoop_import/departments_enclosedby \
--enclosed-by \"
# we have to escape " char with \

# check the data with after enclosed by 
$ hadoop fs -cat /user/cloudera/sqoop_import/departments_enclosedby/part*

# --fields-terminated-by & --lines-terminated-by 
# syntax = --fields-terminated-by <char>
# syntax = --lines-terminated-by <char>

$ sqoop import \
--connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
--username retail_dba \
--password cloudera \
--table departments \
--target-dir /user/cloudera/sqoop_import/departments_enclosedby \
--enclosed-by \"
--fields-terminated-by \|
--lines-terminated-by \:

output = "2"|"Fitness":"3"|"Footwear":"4"|"Apparel":"5"|"Golf":"6"|"Outdoors":"7"|"Fan Shop":"8"|"Big Data":"9"|"IT":"10"|"Finance":"8000"|" TESTING":"9000"|"

# importing into Hive table 

$ sqoop import \
--connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
--username retail_dba \
--password cloudera \
--table departments \
--hive-home /user/hive/warehouse \
--hive-import \
--hive-table departments_test \
--create-hive-table \
--outdir java_files 

# after this login to Hive 
hive> show tables;
hive> select * from departments_test;
hive> describe formatted departments_test; # here we see the field delimiter \u0001 & line.delim \n instead of comma (,) 



# --null-string --> to tell how the nulls' should be represented. NULL will be written for string value
# --null-non-string --> to tell how null non strings should be represented. NULL will be written for non-string value 
# create table departments_test in mysql as select * from departments
mysql> create table departments_test(department_id integer, department_name varchar(30));
# insert null values into departments_test table 
mysql> insert into departments_test values (null, null);
#table output is below with NULL, NULL and run the sqoop import into hive to test --null-string & --null-non-string

department_id | department_name |
+---------------+-----------------+
|             2 | Fitness         |
|             3 | Footwear        |
|             4 | Apparel         |
|             5 | Golf            |
|             6 | Outdoors        |
|             7 | Fan Shop        |
|             8 | Big Data        |
|             9 | IT              |
|            10 | Finance         |
|          8000 |  TESTING        |
|          9000 | testing export  |
|          NULL | NULL         

$ sqoop import \
  --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
  --username=retail_dba \
  --password=cloudera \
  --table departments_test \
  --hive-home /user/hive/warehouse \
  --hive-import \
  --hive-table departments_test \
  --create-hive-table \
  --outdir java_files \
  -m 1 \
  --null-string nvl \
  --null-non-string -1
# after the above script executed successfull, table departments_test will be imported into hive and NULL values represent like below

department_id | department_name |
+---------------+-----------------+
|             2 | Fitness         |
|             3 | Footwear        |
|             4 | Apparel         |
|             5 | Golf            |
|             6 | Outdoors        |
|             7 | Fan Shop        |
|             8 | Big Data        |
|             9 | IT              |
|            10 | Finance         |
|          8000 |  TESTING        |
|          9000 | testing export  |
|          -1   | nvl

************* sqoop export (HDFS --> MySQL)  delimiters ***********************************************************
 # view can give the actual delimiter like $ view part-m-00000

sqoop export \
  --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
  --username=retail_dba \
  --password=cloudera \
  --table departments_test \
  --export-dir /user/hive/warehouse/departments_test \
  --input-fields-terminated-by '\001' \
  --input-lines-terminated-by '\n' \
  --num-mappers 2 \
  --batch \
  --outdir java_files \
  --input-null-string nvl \
  --input-null-non-string -1

# output of the above script,records with department_id=-1,departments_name=nvl record in HDFS will be written as department_id=null, departments_name=null in MySQL

********************* https://www.youtube.com/watch?v=rZP3xkR_0sU - Sqoop File Formats ********************************

--as-avrodatafile
--as-sequesncefile 
--as-textfile
--as-parquetfile (storing data in columnar format)

Default is text file 

sqoop import \
  --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
  --username=retail_dba \
  --password=cloudera \
  --table departments \
  --as-textfile \
  --target-dir=/user/cloudera/departments_text

sqoop import \
  --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
  --username=retail_dba \
  --password=cloudera \
  --table departments \
  --as-sequencefile \
  --target-dir=/user/cloudera/departments_sequencefile

# sequnce file will store as in binary format
# if we use warehouse directory, it will create as departments_sequencefile/departments_sequencefile (directory name twice)
# if we use target directory, it will create as departments_sequencefile (directory name one time )

sqoop import \
  --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
  --username=retail_dba \
  --password=cloudera \
  --table departments \
  --as-avrodatafile \
  --target-dir=/user/cloudera/departments_avrofile

# A file with extension avsc will be created under the home directory(/home/cloudera) from which sqoop import is executed
# Copy avsc file to HDFS location
# Create hive table with LOCATION to /user/cloudera/departments and TBLPROPERTIES pointing to avsc file
hadoop fs -put /home/cloudera/departments.avsc /user/cloudera (copying from home location to HDFS location)
$ hadoop fs -ls /user/cloudera (to see .avsc file copied or not)
# $ ls -ltr (to see the .avsc file)

# creating table in hive as a external table using below script. Login to hive and run the below script

hive> CREATE EXTERNAL TABLE departments
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe'
STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'
OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'
LOCATION 'hdfs:///user/cloudera/departments'
TBLPROPERTIES ('avro.schema.url'='hdfs://quickstart.cloudera/user/cloudera/departments.avsc');

 *** ERROR ****
hive> select * from departments;
OK
Failed with exception java.io.IOException:java.io.IOException: Not a data file.
Time taken: 0.066 seconds
*** QUESTION ****

Though I used the table departments_avrofile , it created departments.avsc file under/home/cloudera -- Not sure why. 

************************** https://www.youtube.com/watch?v=PXBbFz9Ty8I - sqoop job & sqoop merge *********************************

# IMP: before import there is a blank space required 

sqoop job --create sqoop_job \
  -- import \
  --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
  --username=retail_dba \
  --password=cloudera \
  --table departments \
  --target-dir /user/hive/warehouse/retail_ods.db/departments \
  --append \
  --fields-terminated-by '|' \
  --lines-terminated-by '\n' \
  --check-column "department_id" \
  --incremental append \
  --last-value 7 \
  --outdir java_files

sqoop job --list 
# to show the list of the jobs created 

sqoop job --show sqoop_job
# to show details of sqoop_job 

sqoop job --exec sqoop_job
# to execute the job "sqoop_job" which is created using the above script

********************************* sqoop merge ********************************************************

# In Hive & HDFS , we cannot udpate the data. We can only append the data 


--Merge process begins
hadoop fs -mkdir /user/cloudera/sqoop_merge

--Initial load
sqoop import \
  --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
  --username=retail_dba \
  --password=cloudera \
  --table departments \
  --as-textfile \
  --target-dir=/user/cloudera/sqoop_merge/departments

--Validate
sqoop eval --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
  --username retail_dba \
  --password cloudera \
  --query "select * from departments" 

hadoop fs -cat /user/cloudera/sqoop_merge/departments/part*

--update
sqoop eval --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
  --username retail_dba \
  --password cloudera \
  --query "update departments set department_name='Testing Merge' where department_id = 9000"

--Insert
sqoop eval --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
  --username retail_dba \
  --password cloudera \
  --query "insert into departments values (10000, 'Inserting for merge')"

sqoop eval --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
  --username retail_dba \
  --password cloudera \
  --query "select * from departments"

--New load
sqoop import \
  --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
  --username=retail_dba \
  --password=cloudera \
  --table departments \
  --as-textfile \
  --target-dir=/user/cloudera/sqoop_merge/departments_delta \
  --where "department_id >= 9000"

hadoop fs -cat /user/cloudera/sqoop_merge/departments_delta/part*

--Merge
sqoop merge --merge-key department_id \
  --new-data /user/cloudera/sqoop_merge/departments_delta \
  --onto /user/cloudera/sqoop_merge/departments \
  --target-dir /user/cloudera/sqoop_merge/departments_stage \
  --class-name departments \
  --jar-file <get_it_from_last_import>

# previous sqoop import jar file = /tmp/sqoop-cloudera/compile/cdebee1910df61dad3f8b7a2cf63afbe/departments.jar

hadoop fs -cat /user/cloudera/sqoop_merge/departments_stage/part*

--Delete old directory
hadoop fs -rm -R /user/cloudera/sqoop_merge/departments

--Move/rename stage directory to original directory
hadoop fs -mv /user/cloudera/sqoop_merge/departments_stage /user/cloudera/sqoop_merge/departments 

--Validate that original directory have merged data
hadoop fs -cat /user/cloudera/sqoop_merge/departments/part*

--Merge process ends


****************************************  For help *******************************************************

$ sqoop import --help # to get the help on sqoop import command


$$$$$$$$$$$$$$$$$$$$$$$$$$$$ COPY FROM DGADIRAJU GITHUB ACCOUNT  $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$

sqoop list-databases \
  --connect "jdbc:mysql://quickstart.cloudera:3306" \
  --username retail_dba \
  --password cloudera

sqoop list-tables \ 
  --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
  --username retail_dba \
  --password cloudera

sqoop eval \
  --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
  --username retail_dba \
  --password cloudera \
  --query "select count(1) from order_items"

-- Reference: http://www.cloudera.com/content/cloudera/en/developers/home/developer-admin-resources/get-started-with-hadoop-tutorial/exercise-1.html
sqoop import-all-tables \
  -m 12 \
  --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
  --username=retail_dba \
  --password=cloudera \
  --as-avrodatafile \
  --warehouse-dir=/user/hive/warehouse/retail_stage.db

--Default
sqoop import \
  --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
  --username=retail_dba \
  --password=cloudera \
  --table departments \
  --as-textfile \
  --target-dir=/user/cloudera/departments

sqoop import \
  --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
  --username=retail_dba \
  --password=cloudera \
  --table departments \
  --as-sequencefile \
  --target-dir=/user/cloudera/departments

sqoop import \
  --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
  --username=retail_dba \
  --password=cloudera \
  --table departments \
  --as-avrodatafile \
  --target-dir=/user/cloudera/departments

-- A file with extension avsc will be created under the directory from which sqoop import is executed
-- Copy avsc file to HDFS location
-- Create hive table with LOCATION to /user/cloudera/departments and TBLPROPERTIES pointing to avsc file
hadoop fs -put sqoop_import_departments.avsc /user/cloudera

CREATE EXTERNAL TABLE departments
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe'
STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'
OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'
LOCATION 'hdfs:///user/cloudera/departments'
TBLPROPERTIES ('avro.schema.url'='hdfs://quickstart.cloudera/user/cloudera/sqoop_import_departments.avsc');


-- It will create tables in default database in hive
-- Using snappy compression
-- As we have imported all tables before make sure you drop the directories
-- Launch hive drop all tables
drop table departments;
drop table categories;
drop table products;
drop table orders;
drop table order_items;
drop table customers;

-- Dropping directories, in case your hive database/tables in consistent state
hadoop fs -rm -R /user/hive/warehouse/departments
hadoop fs -rm -R /user/hive/warehouse/categories
hadoop fs -rm -R /user/hive/warehouse/products
hadoop fs -rm -R /user/hive/warehouse/orders 
hadoop fs -rm -R /user/hive/warehouse/order_itmes
hadoop fs -rm -R /user/hive/warehouse/customers

sqoop import-all-tables \
  --num-mappers 1 \
  --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
  --username=retail_dba \
  --password=cloudera \
  --hive-import \
  --hive-overwrite \
  --create-hive-table \
  --compress \
  --compression-codec org.apache.hadoop.io.compress.SnappyCodec \
  --outdir java_files

sudo -u hdfs hadoop fs -mkdir /user/cloudera/retail_stage
sudo -u hdfs hadoop fs -chmod +rw /user/cloudera/retail_stage
hadoop fs -copyFromLocal ~/*.avsc /user/cloudera/retail_stage

-- Basic import
sqoop import \
  --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
  --username=retail_dba \
  --password=cloudera \
  --table departments \
  --target-dir /user/cloudera/departments 

-- Boundary Query and columns
sqoop import \
  --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
  --username=retail_dba \
  --password=cloudera \
  --table departments \
  --target-dir /user/cloudera/departments \
  -m 2 \
  --boundary-query "select 2, 8 from departments limit 1" \
  --columns department_id,department_name

-- query and split-by
sqoop import \
  --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
  --username=retail_dba \
  --password=cloudera \
  --query="select * from orders join order_items on orders.order_id = order_items.order_item_order_id where \$CONDITIONS" \
  --target-dir /user/cloudera/order_join \
  --split-by order_id \
  --num-mappers 4

-- Copying into existing table or directory (append)
-- Customizing number of threads (num-mappers)
-- Changing delimiter
sqoop import \
  --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
  --username=retail_dba \
  --password=cloudera \
  --table departments \
  --target-dir /user/hive/warehouse/retail_ods.db/departments \
  --append \
  --fields-terminated-by '|' \
  --lines-terminated-by '\n' \
  --num-mappers 1 \
  --outdir java_files

-- Importing table with out primary key using multiple threads (split-by)
-- When using split-by, using indexed column is highly desired
-- If the column is not indexed then performance will be bad 
-- because of full table scan by each of the thread
sqoop import \
  --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
  --username=retail_dba \
  --password=cloudera \
  --table departments \
  --target-dir /user/hive/warehouse/retail_ods.db/departments \
  --append \
  --fields-terminated-by '|' \
  --lines-terminated-by '\n' \
  --split-by department_id \
  --outdir java_files

-- Getting delta (--where)
sqoop import \
  --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
  --username=retail_dba \
  --password=cloudera \
  --table departments \
  --target-dir /user/hive/warehouse/retail_ods.db/departments \
  --append \
  --fields-terminated-by '|' \
  --lines-terminated-by '\n' \
  --split-by department_id \
  --where "department_id > 7" \
  --outdir java_files

-- Incremental load
sqoop import \
  --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
  --username=retail_dba \
  --password=cloudera \
  --table departments \
  --target-dir /user/hive/warehouse/retail_ods.db/departments \
  --append \
  --fields-terminated-by '|' \
  --lines-terminated-by '\n' \
  --check-column "department_id" \
  --incremental append \
  --last-value 7 \
  --outdir java_files

sqoop job --create sqoop_job \
  -- import \
  --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
  --username=retail_dba \
  --password=cloudera \
  --table departments \
  --target-dir /user/hive/warehouse/retail_ods.db/departments \
  --append \
  --fields-terminated-by '|' \
  --lines-terminated-by '\n' \
  --check-column "department_id" \
  --incremental append \
  --last-value 7 \
  --outdir java_files

sqoop job --list

sqoop job --show sqoop_job

sqoop job --exec sqoop_job

-- Hive related
-- Overwrite existing data associated with hive table (hive-overwrite)
sqoop import \
  --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
  --username=retail_dba \
  --password=cloudera \
  --table departments \
  --fields-terminated-by '|' \
  --lines-terminated-by '\n' \
  --hive-home /user/hive/warehouse/retail_ods.db \
  --hive-import \
  --hive-overwrite \
  --hive-table departments \
  --outdir java_files

--Create hive table example
sqoop import \
  --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
  --username=retail_dba \
  --password=cloudera \
  --table departments \
  --fields-terminated-by '|' \
  --lines-terminated-by '\n' \
  --hive-home /user/hive/warehouse \
  --hive-import \
  --hive-table departments_test \
  --create-hive-table \
  --outdir java_files

--Connect to mysql and create database for reporting database
--user:root, password:cloudera
mysql -u root -p
create database retail_rpt_db;
grant all on retail_rpt_db.* to retail_dba;
flush privileges;
use retail_rpt_db;
create table departments as select * from retail_db.departments where 1=2;
exit;

--For certification change database name retail_rpt_db to retail_db
sqoop export --connect "jdbc:mysql://quickstart.cloudera:3306/retail_rpt_db" \
       --username retail_dba \
       --password cloudera \
       --table departments \
       --export-dir /user/hive/warehouse/retail_ods.db/departments \
       --input-fields-terminated-by '|' \
       --input-lines-terminated-by '\n' \
       --num-mappers 2 \
       --batch \
       --outdir java_files

sqoop export --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
  --username retail_dba \
  --password cloudera \
  --table departments \
  --export-dir /user/cloudera/sqoop_import/departments_export \
  --batch \
  --outdir java_files \
  -m 1 \
  --update-key department_id \
  --update-mode allowinsert

sqoop export --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
  --username retail_dba \
  --password cloudera \
  --table departments_test \
  --export-dir /user/hive/warehouse/departments_test \
  --input-fields-terminated-by '\001' \
  --input-lines-terminated-by '\n' \
  --num-mappers 2 \
  --batch \
  --outdir java_files \
  --input-null-string nvl \
  --input-null-non-string -1

--Merge process begins
hadoop fs -mkdir /user/cloudera/sqoop_merge

--Initial load
sqoop import \
  --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
  --username=retail_dba \
  --password=cloudera \
  --table departments \
  --as-textfile \
  --target-dir=/user/cloudera/sqoop_merge/departments

--Validate
sqoop eval --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
  --username retail_dba \
  --password cloudera \
  --query "select * from departments" 

hadoop fs -cat /user/cloudera/sqoop_merge/departments/part*

--update
sqoop eval --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
  --username retail_dba \
  --password cloudera \
  --query "update departments set department_name='Testing Merge' where department_id = 9000"

--Insert
sqoop eval --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
  --username retail_dba \
  --password cloudera \
  --query "insert into departments values (10000, 'Inserting for merge')"

sqoop eval --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
  --username retail_dba \
  --password cloudera \
  --query "select * from departments"

--New load
sqoop import \
  --connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
  --username=retail_dba \
  --password=cloudera \
  --table departments \
  --as-textfile \
  --target-dir=/user/cloudera/sqoop_merge/departments_delta \
  --where "department_id >= 9000"

hadoop fs -cat /user/cloudera/sqoop_merge/departments_delta/part*

--Merge
sqoop merge --merge-key department_id \
  --new-data /user/cloudera/sqoop_merge/departments_delta \
  --onto /user/cloudera/sqoop_merge/departments \
  --target-dir /user/cloudera/sqoop_merge/departments_stage \
  --class-name departments \
  --jar-file <get_it_from_last_import>

hadoop fs -cat /user/cloudera/sqoop_merge/departments_stage/part*

--Delete old directory
hadoop fs -rm -R /user/cloudera/sqoop_merge/departments

--Move/rename stage directory to original directory
hadoop fs -mv /user/cloudera/sqoop_merge/departments_stage /user/cloudera/sqoop_merge/departments 

--Validate that original directory have merged data
hadoop fs -cat /user/cloudera/sqoop_merge/departments/part*

--Merge process ends






















